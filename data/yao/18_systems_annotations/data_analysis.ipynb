{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alex', 99)\n",
      "('elizabeth', 98)\n",
      "('rachel', 99)\n",
      "('marcus', 99)\n",
      "('vishnesh', 100)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "annotator_list = ['alex', 'elizabeth', 'rachel', 'marcus', 'vishnesh']\n",
    "annotator_data_list = []\n",
    "total_original_sentences = []\n",
    "annotator_original_sentences_list = []\n",
    "for annotator_name in annotator_list:\n",
    "    directory = '' + annotator_name + '/'\n",
    "    original_sentences = []\n",
    "    with open(directory + \"total.json\") as f:\n",
    "        data = json.load(f)\n",
    "        # itereate data to get total original sentences\n",
    "        for data_instance in data:\n",
    "            total_original_sentences.append(data_instance['Original'])\n",
    "            original_sentences.append(data_instance['Original'])\n",
    "        annotator_original_sentences_list.append(original_sentences)\n",
    "        annotator_data_list.append(data)\n",
    "    print((annotator_name, len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read \"adding_new_one.json\"\n",
    "with open(\"adding_new_one.json\") as f:\n",
    "    the_new_one = json.load(f)[0]\n",
    "    the_new_one_original = the_new_one['Original']\n",
    "\n",
    "be_replaced_one_original = \"He died on May 29, 1518 in Madrid, Spain and was buried in the church of San Benito d'Alcantara.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the annotation of be_replaced_one_original from each annotator's annotations\n",
    "for i, annotator_name in enumerate(annotator_list):\n",
    "    annotator_data = annotator_data_list[i]\n",
    "    annotator_original_sentences = annotator_original_sentences_list[i]\n",
    "    for j, data_instance in enumerate(annotator_data):\n",
    "        if data_instance['Original'] == be_replaced_one_original:\n",
    "            annotator_data.pop(j)\n",
    "            annotator_original_sentences.pop(j)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_original_sentences = list(set(total_original_sentences))\n",
    "# remove be_replaced_one_original from total_original_sentences\n",
    "total_original_sentences.remove(be_replaced_one_original)\n",
    "# add the_new_one_original to total_original_sentences\n",
    "total_original_sentences.append(the_new_one_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save total original sentences to file\n",
    "with open('total_100_original_sentences.json', 'w') as f:\n",
    "    json.dump(total_original_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vishnesh_annotations = annotator_data_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, annotator_name in enumerate(annotator_list):\n",
    "    operations = [\"Deletions\", \"Paraphrases\", \"Splittings\"]\n",
    "    annotations = annotator_data_list[i]\n",
    "    missing_originals = list(set(total_original_sentences) - set(annotator_original_sentences_list[i]))\n",
    "    for missing_original in missing_originals:\n",
    "        if missing_original == the_new_one_original:\n",
    "            new_one_copy = copy.deepcopy(the_new_one)\n",
    "            new_one_copy['Worker'] = annotator_name\n",
    "            for j, operation in enumerate(operations):\n",
    "                    for k, edit in enumerate(new_one_copy[operation]):\n",
    "                        # add -1 to the edit at front\n",
    "                        edit_temp = [-1] + edit\n",
    "                        new_one_copy[operation][k] = edit_temp\n",
    "            annotator_data_list[i].append(new_one_copy)\n",
    "        else:\n",
    "            for instance in vishnesh_annotations:\n",
    "                original_sentence = instance['Original']\n",
    "                if original_sentence == missing_original:\n",
    "                    # deep copy instance as instance_copy\n",
    "                    instance_copy = copy.deepcopy(instance)\n",
    "                    instance_copy['Worker'] = annotator_name\n",
    "                    for operation in operations:\n",
    "                        for j, edit in enumerate(instance_copy[operation]):\n",
    "                            instance_copy[operation][j][0] = -1\n",
    "                    annotator_data_list[i].append(instance_copy)\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alex', 100)\n",
      "('elizabeth', 100)\n",
      "('rachel', 100)\n",
      "('marcus', 100)\n",
      "('vishnesh', 100)\n"
     ]
    }
   ],
   "source": [
    "for i, annotator_name in enumerate(annotator_list):\n",
    "    print((annotator_name, len(annotator_data_list[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign ids to instances\n",
    "for i, annotator_name in enumerate(annotator_list):\n",
    "    for annotation in annotator_data_list[i]:\n",
    "        original_sentence = annotation['Original']\n",
    "        annotation['ID'] = total_original_sentences.index(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add three new references into the data\n",
    "\n",
    "# read three_new_systems_input.json\n",
    "with open('../systems/three_new_systems_input.json') as f:\n",
    "    three_new_systems_input = json.load(f)\n",
    "\n",
    "for annotation in three_new_systems_input:\n",
    "    operations = [\"Deletions\", \"Paraphrases\", \"Splittings\"]\n",
    "    original_sentence = annotation['Original']\n",
    "    for i, annotator_name in enumerate(annotator_list):\n",
    "        annotator_annotations = annotator_data_list[i]\n",
    "        for annotator_annotation in annotator_annotations:\n",
    "            if annotator_annotation['Original'] == original_sentence:\n",
    "                for j, operation in enumerate(operations):\n",
    "                    for k, edit in enumerate(annotation[operation]):\n",
    "                        # add -1 to the edit at front\n",
    "                        edit_temp = [-1] + edit\n",
    "                        # add to annotator_annotation\n",
    "                        annotator_annotation[operation].append(edit_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33 34\n",
      "33 33 34\n",
      "33 33 34\n",
      "33 33 34\n",
      "33 33 34\n"
     ]
    }
   ],
   "source": [
    "# split each annotator's data into 3 batches, id < 33, 33 <= id < 66, 66 <= id < 100\n",
    "for i, annotator_name in enumerate(annotator_list):\n",
    "    annotations = annotator_data_list[i]\n",
    "    # sort annotations by id\n",
    "    annotations.sort(key=lambda x: x['ID'])\n",
    "    annotations_batch_1 = []\n",
    "    annotations_batch_2 = []\n",
    "    annotations_batch_3 = []\n",
    "    for annotation in annotations:\n",
    "        if annotation['ID'] < 33:\n",
    "            annotations_batch_1.append(annotation)\n",
    "        elif annotation['ID'] < 66:\n",
    "            annotations_batch_2.append(annotation)\n",
    "        else:\n",
    "            annotations_batch_3.append(annotation)\n",
    "    print(len(annotations_batch_1), len(annotations_batch_2), len(annotations_batch_3))\n",
    "    with open(annotator_name + '/new_batches/batch_1.json', 'w') as f:\n",
    "        json.dump(annotations_batch_1, f)\n",
    "    with open(annotator_name + '/new_batches/batch_2.json', 'w') as f:\n",
    "        json.dump(annotations_batch_2, f)\n",
    "    with open(annotator_name + '/new_batches/batch_3.json', 'w') as f:\n",
    "        json.dump(annotations_batch_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read batch 3 from elizabeth\n",
    "with open('vishnesh/new_batches/batch_3.json') as f:\n",
    "    elizabeth_batch_3 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, annotation in enumerate(elizabeth_batch_3):\n",
    "    operations = [\"Deletions\", \"Paraphrases\", \"Splittings\"]\n",
    "    new_system_num = 0\n",
    "    for operation in operations:\n",
    "        for i, edit in enumerate(annotation[operation]):\n",
    "            system = edit[2]\n",
    "            if \"new_systems\" in system:\n",
    "                new_system_num += 1\n",
    "    if new_system_num != 3:\n",
    "        print(annotation['Original'], annotation[\"ID\"], j, new_system_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('paraphrase')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51d4b053945bb951b16f24566ff3322f1cf2f2e71937c6e2fa134fa8352168a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
